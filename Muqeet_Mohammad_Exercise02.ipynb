{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaX7-bOhcwkT"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwe6cp-3cwkX"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EYONqMucwkY"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection:**\n",
        "\n",
        "To answer this research question, we would need to collect longitudinal data on individuals' social media usage and their mental health indicators over time. This data could include:\n",
        "\n",
        "**Social Media Usage Data:**\n",
        "\n",
        "Frequency of social media use (e.g., daily, weekly)\n",
        "\n",
        "Duration of social media use per session\n",
        "\n",
        "Types of social media platforms used (e.g., Facebook, Instagram, Twitter)\n",
        "\n",
        "Content consumption patterns (e.g., scrolling, posting, interacting with others)\n",
        "\n",
        "Engagement metrics (e.g., likes, comments, shares)\n",
        "\n",
        "Self-reported reasons for using social media (e.g., social connection, entertainment, information seeking)\n",
        "\n",
        "**Mental Health and Well-being Indicators:**\n",
        "\n",
        "Self-reported mental health assessments (e.g., depression, anxiety)\n",
        "\n",
        "Perceived stress levels\n",
        "\n",
        "Quality of sleep\n",
        "\n",
        "Self-esteem and self-worth\n",
        "\n",
        "Social support and loneliness measures\n",
        "\n",
        "Demographic and Contextual Variables:\n",
        "\n",
        "\n",
        "Age, gender, and other demographic information\n",
        "\n",
        "Socioeconomic status\n",
        "\n",
        "Life events and stressors\n",
        "\n",
        "Other relevant contextual factors (e.g., pandemic-related factors, changes in job status)\n",
        "\n",
        "**Data Collection Steps:**\n",
        "\n",
        "Ethical Considerations: Ensure that the data collection process adheres to ethical guidelines and obtains informed consent from participants. Protect participants' privacy and confidentiality throughout the data collection and analysis process.\n",
        "\n",
        "Participant Recruitment: Recruit a diverse sample of participants representing different demographics (e.g., age, gender, socioeconomic status) to capture a comprehensive understanding of the relationship between social media usage and mental health.\n",
        "\n",
        "Data Collection Instruments: Develop or utilize validated survey instruments to collect data on social media usage, mental health indicators, and demographic/contextual variables. These instruments can be administered through online surveys or mobile applications for convenient data collection.\n",
        "\n",
        "Longitudinal Data Collection: Collect data at multiple time points over an extended period (e.g., months or years) to capture changes in social media usage and mental health outcomes over time. Ensure consistent data collection procedures and minimize attrition to maintain data quality.\n",
        "\n",
        "Data Storage: Store collected data securely in compliance with data protection regulations. Utilize encrypted storage methods and access controls to safeguard participant information."
      ],
      "metadata": {
        "id": "QieWNQ9Gc_7n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idheF8ckcwkZ"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcq4-FbEcwkZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Simulate data for social media usage\n",
        "social_media_data = {\n",
        "    'user_id': np.arange(1, 1001),\n",
        "    'frequency': np.random.choice(['daily', 'weekly', 'monthly'], size=1000),\n",
        "    'duration_minutes': np.random.randint(1, 601, size=1000),\n",
        "    'platform': np.random.choice(['Facebook', 'Instagram', 'Twitter', 'Snapchat'], size=1000),\n",
        "    'engagement_likes': np.random.randint(0, 100, size=1000),\n",
        "    'engagement_comments': np.random.randint(0, 50, size=1000),\n",
        "    'engagement_shares': np.random.randint(0, 20, size=1000),\n",
        "    'reason': np.random.choice(['social connection', 'entertainment', 'information seeking'], size=1000)\n",
        "}\n",
        "\n",
        "# Simulate data for mental health indicators\n",
        "mental_health_data = {\n",
        "    'user_id': np.arange(1, 1001),\n",
        "    'depression_score': np.random.randint(0, 11, size=1000),\n",
        "    'anxiety_score': np.random.randint(0, 11, size=1000),\n",
        "    'stress_level': np.random.randint(0, 11, size=1000),\n",
        "    'quality_of_sleep': np.random.randint(0, 11, size=1000),\n",
        "    'self_esteem': np.random.randint(0, 11, size=1000),\n",
        "    'social_support': np.random.randint(0, 11, size=1000)\n",
        "}\n",
        "\n",
        "# Combine social media and mental health data into a single DataFrame\n",
        "df = pd.DataFrame(social_media_data)\n",
        "df = df.merge(pd.DataFrame(mental_health_data), on='user_id')\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv('social_media_mental_health_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhw8sur7cwka"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7jdC980cwka",
        "outputId": "de6ae206-651d-4989-a851-8c04daaad9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n",
            "500\n",
            "500\n",
            "500\n",
            "                                               Title  \\\n",
            "0                              Information Retrieval   \n",
            "1                       Modern Information Retrieval   \n",
            "2                      Private Information Retrieval   \n",
            "3           An Introduction to Information Retrieval   \n",
            "4  Naive (Bayes) at Forty: The Independence Assum...   \n",
            "\n",
            "                                   Authors  Year  \\\n",
            "0                        C.J.vanRijsbergen  1979   \n",
            "1  RicardoBaeza-Yates,BerthierRibeiro-Neto  1999   \n",
            "2                          BennyChor,etal.     0   \n",
            "3               ChristopherD.Manning,etal.  2007   \n",
            "4                             DavidD.Lewis  1998   \n",
            "\n",
            "                                            Abstract  \n",
            "0                                        \"...   ...\"  \n",
            "1  \"... Information retrieval (IR) has changed co...  \n",
            "2  \"...   We describe schemes that enable a user ...  \n",
            "3                                        \"...   ...\"  \n",
            "4  \"... The naive Bayes classifier, currently exp...  \n",
            "(500, 4)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import re\n",
        "total_count = 0\n",
        "count = 0\n",
        "\n",
        "result_df = {\"Title\":[], \"Authors\":[], \"Year\":[], \"Abstract\":[]}\n",
        "main_url = \"https://citeseerx.ist.psu.edu/search?q=information+retrieval&t=doc&sort=rlv&start={}\"\n",
        "for page_num in range(0, 5000, 10):\n",
        "    #print(main_url.format(page_num))\n",
        "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    url1 = urlopen(link1)\n",
        "\n",
        "    data1 = url1.read()\n",
        "    data1_soup = BeautifulSoup(data1)\n",
        "    #print(data1_soup)\n",
        "\n",
        "    for i in data1_soup.find_all(\"a\", attrs={'class':'remove doc_details'}):\n",
        "        #print(i.text.strip())\n",
        "        result_df[\"Title\"].append(i.text.strip())\n",
        "\n",
        "    for i in data1_soup.find_all(\"div\", attrs={'class':'pubinfo'}):\n",
        "        #print(len(i.find_all(\"span\")))\n",
        "        if len(i.find_all(\"span\")) < 2:\n",
        "            result_df[\"Authors\"].append(i.find_all(\"span\")[0].text.replace(\" \", \"\").replace(\"\\n\", \" \").split()[1])\n",
        "            result_df['Year'].append(0)\n",
        "\n",
        "        elif len(i.find_all(\"span\")) > 2:\n",
        "            result_df[\"Authors\"].append(i.find_all(\"span\")[0].text.replace(\" \", \"\").replace(\"\\n\", \" \").split()[1])\n",
        "            result_df['Year'].append(i.find_all(\"span\")[2].text.split()[1])\n",
        "\n",
        "        else:\n",
        "            result_df[\"Authors\"].append(i.find_all(\"span\")[0].text.replace(\" \", \"\").replace(\"\\n\", \" \").split()[1])\n",
        "            result_df['Year'].append(i.find_all(\"span\")[1].text.split()[1])\n",
        "    for i in data1_soup.find_all(\"div\", attrs={'class':'snippet'}):\n",
        "        #for j in i.find_all(\"span\"):\n",
        "        #print(i.text.replace(\" \", \"\"))\n",
        "        #print(i.text)\n",
        "        result_df[\"Abstract\"].append(i.text)\n",
        "    #print(title)\n",
        "\n",
        "print(len(result_df[\"Title\"]))\n",
        "print(len(result_df[\"Authors\"]))\n",
        "print(len(result_df[\"Year\"]))\n",
        "print(len(result_df[\"Abstract\"]))\n",
        "df = pd.DataFrame(result_df)\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "#df['Year'] = df['Year'].astype(\"int\")\n",
        "#df[df['Year'] >= 2014]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqPWccktcwkb"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5hsMwKfcwkb",
        "outputId": "57e8c797-9a1f-4fe5-f88b-29579a234b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RT @polo_kimani: Brudda,usiwahi dhania hawa chiles wa Twitter for iphone wako out of your league,( financially they are) but the iphone huw…\n",
            "2022-09-25 15:16:34+00:00\n",
            "\n",
            "@thismikael @KnobSlobberz @CaliMOfficial_ @ATLONIKA How she aggressively throwing that big burnt brownie on my iPho… https://t.co/LjLmQHfXT8\n",
            "2022-09-25 15:16:30+00:00\n",
            "\n",
            "RT @nftbadger: When you drop your friend’s new iphone https://t.co/aZvg2VcdV8\n",
            "2022-09-25 15:16:29+00:00\n",
            "\n",
            "RT @nftbadger: When you drop your friend’s new iphone https://t.co/aZvg2VcdV8\n",
            "2022-09-25 15:16:29+00:00\n",
            "\n",
            "RT @nftbadger: When you drop your friend’s new iphone https://t.co/aZvg2VcdV8\n",
            "2022-09-25 15:16:26+00:00\n",
            "\n",
            "RT @alhajinuell: I never see affiliate marketer wey buy iPhone 14. I thought y’all were making over 200k everyday while we were sleeping? 😂…\n",
            "2022-09-25 15:16:26+00:00\n",
            "\n",
            "I'm using #Watusi on iPhone by @FouadRaheb to add new features for #WhatsApp! https://t.co/aRK1rm8WFC\n",
            "2022-09-25 15:16:24+00:00\n",
            "\n",
            "RT @nftbadger: When you drop your friend’s new iphone https://t.co/aZvg2VcdV8\n",
            "2022-09-25 15:16:24+00:00\n",
            "\n",
            "Lung cancer to the right of B3 Skid and pup's Step sister 8: The twitter bot For iPhone: Beta version\n",
            "2022-09-25 15:16:24+00:00\n",
            "\n",
            "Thanks goes out to @TwitterSupport for fixing the #Twitter #iPhone app, \"Tweet Activity\" capability\n",
            "2022-09-25 15:16:21+00:00\n",
            "\n",
            "RT @OGBdeyforyou: @blacksherif_ @Mbahdeyforyou Them don promise your babe iPhone 14?\n",
            "2022-09-25 15:16:20+00:00\n",
            "\n",
            "Had a feeling the iPhone 14 Pro Max would win this.\n",
            "\n",
            "Only problem is the Pixel 6 Pro takes the best still images 🤔\n",
            "2022-09-25 15:16:19+00:00\n",
            "\n",
            "RT @appleinsider: The 2023 fall updates to the #iPhone lineup could introduce a new iPhone 15 Ultra model, replacing the usual Pro Max vari…\n",
            "2022-09-25 15:16:18+00:00\n",
            "\n",
            "iPhone users - how much proprietary intelligence is in a Lightning charge cable? Have recently had both a genuine c… https://t.co/agXaLJd7Uu\n",
            "2022-09-25 15:16:18+00:00\n",
            "\n",
            "RT @mhauken: 9 iPhone apps that can double your productivity:\n",
            "2022-09-25 15:16:16+00:00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "key= '2Em7SxlX9jPMfL4x97r3zMO0x'\n",
        "secret= 'sVbJzekKuiAgq83Y7gCwNVbSowqQokGVzWexKHl2cXIPceWtSd'\n",
        "token= '1439767876962029572-uUMt8oWRyzj9ilE5zk4uYbL93sCMPT'\n",
        "access= 'oydIGymn9bS767FVEMawE9GyGAnMmBJfaY2XXKmHnmliF'\n",
        "\n",
        "# Creating the authentication object\n",
        "auth = tweepy.OAuthHandler(key, secret)\n",
        "\n",
        "# Setting your access token and secret\n",
        "auth.set_access_token(token, access)\n",
        "\n",
        "# Creating the API object while passing in auth information\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "for tweets in api.search_tweets(q=\"iphone\", lang=\"en\"):\n",
        "    print(tweets.text)\n",
        "    print(tweets.created_at)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ],
      "metadata": {
        "id": "9lX-3NDed0vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got no issue."
      ],
      "metadata": {
        "id": "j-Wp2oiBd54Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "Xs-rUnssd_iJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "WmYWxMeReBFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Learning Experience:**\n",
        "\n",
        "Overall, working on web scraping tasks provided a valuable learning experience. I gained a better understanding of HTML structure, CSS selectors, and how to navigate and extract data from web pages using libraries like BeautifulSoup in Python. Learning about APIs and how to interact with them using tools like Tweepy for Twitter data collection was particularly beneficial. Additionally, understanding the importance of respecting website terms of service and handling rate limits was crucial in ensuring ethical data collection practices.\n",
        "\n",
        "**Challenges Encountered:**\n",
        "\n",
        "One challenge I encountered was handling dynamic content loaded via JavaScript on some websites. This required me to use more advanced techniques like Selenium WebDriver to interact with the page and extract data. Another challenge was dealing with rate limits and ensuring that my scraping activities did not overwhelm the servers or violate website policies. By implementing rate limiting and error handling mechanisms, I was able to overcome these challenges effectively.\n",
        "\n",
        "**Relevance to My Field of Study:**\n",
        "\n",
        "As a student of thi field, the ability to gather and analyze data from online sources is incredibly relevant. Web scraping allows me to collect large datasets for analysis, conduct sentiment analysis on social media data, track trends, and gather insights into user behavior. This skill enables me to augment traditional research methods with data-driven approaches and extract valuable information from diverse online sources. Overall, web scraping and data collection techniques enhance my ability to conduct comprehensive and insightful research in my field."
      ],
      "metadata": {
        "id": "q06b_ZbZeDQ1"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}